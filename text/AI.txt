and sentence level (e.g.,
semantic classification). The BERT model is then extended
to pre-train on multi-lingual data [2]. Our paper builds on
the BERT model to capture structure in both the linguistic
and visual domains.

  

 Image and video captioning. There has been much re-
cent work on image captioning (seee-g.,[!1, 8 15]), which
is a model of the form p(y|x), where y is the manually pro-
 vided caption and 2 isthe image. There has also been some
work on video captioning, using either manually provided
temporal segmentation or estimated segmentations (see e.g.
{20, 29). We use our joint p(z,y) model and apply it to

 = 2 B 

 V Draw Y  F Highlight VY < Erase

 where 6y(2) isthe 'th potential function, with parameters
8, and Z is the partion function

 The above model is permutation invariant. In order 10
capture order information, we can  tag  each word with its
Position in the sentence. The BERT model leams an embed-
Ging fr each of the word tokens, as well as for these tags,
and then sums the embedding vectors to get a continuous
representation for each token. The log potential (energy)
functions for each locaton are defined by

 log du (2|9) = 27 fo(e)

 where 2 is a one-hot vector for the Ith token (and its tag),
and

By = (@1y.-.,211) MASK, 2141,-.-522)
 The function f(2\,) is a multi-layer bidirectional trans-
former model [28] that takes an L x Dy tensor, contain-
ing the D,-dimensional embedding vectors corresponding
to n\,, and returns an L x Dz tensor, where Dz is the size
of the output of each transformer node. See [6] for details
 The mode is trained to approximately maximize the pseudo
log-likelihood

L

 L(0) = Benn Y log p(2ilai8)

   

  

 In practice, we can stochastically optimize the logloss
(computed from the softmax predicted by the f function)
by sampling locations as well as training sentences.

BERT can be extended to model two sentences by con-
catenating them together. However, we are often not only

 ve

  

 @

  

  

 a

 Pee)
 