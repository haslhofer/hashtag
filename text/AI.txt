 TA
PZ tant to leverage the abundance of unlabeled data avail- but this is hard to acquire at scale. Consequently there has
Sable on platforms like YouTube. Whereas most existing been a lot of recent interest in  self supervised learning ,
<2) approaches learn low-level representations, we propose a where we train a model on vatious  proxy tasks, which we
So Hee CE LR Meee ces | le Glcahu ues dias bee
1) pica cay Cnlitk eapeiiod Tic parteulde Wapiea vou har ear'be asc Gguurcain taaed Acide viety

 by its recent success in language modeling, we build upon
the BERT model to learn bidirectional joint distributions
over sequences of visual and linguistic tokens, derived from
vector quantization of video data and off-the-shelf speech
recognition outputs, respectively. We use VideoBERT in nu-
 merous tasks, including action classification and video cap-
tioning. We show that it can be applied directly 10 open-
vocabulary classification, and confirm that large amounts
of training data and cross-modal information are critical 10
performance. Furthermore, we outperform the state-of-the-
 art on video captioning, and quantitative results verify that
the model learns high-level semantic features.

 of such proxy tasks have been proposed in the image and
video domains. However, most of these methods focus on
low level features (cg., textures) and short temporal scales
(g., motion patterns that last a second or less). We are in-
terested in discovering high-level semantic features which
correspond to actions and events that unfold over longer
time scales (e.g. minutes) since such representations would
be useful for various video understanding tasks.

In this paper, we exploit the key insight that human
language has evolved words to describe high-level objects
and events, and thus provides a natural source of  self 
supervision. In particular, we present a simple way to
 model the relationship between the visual domain and the

  

  

  

 Seon aa

 cd

 Dy one)

 cy
SADE)

  
 